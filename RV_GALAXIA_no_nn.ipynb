{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML to find radial velocities using GALAXIA sim with Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import sys\n",
    "import gzip\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.stats import norm\n",
    "\n",
    "matplotlib.rcParams.update({'font.family':'cmr10','font.size': 13})\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "matplotlib.rcParams['axes.labelsize']=15\n",
    "plt.rcParams['figure.figsize']=(4,4)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "plt.rcParams['mathtext.fontset'] = 'cm'\n",
    "plt.rcParams['mathtext.rm'] = 'serif'\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['xtick.top'] = True\n",
    "plt.rcParams['ytick.right'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load('/tigress/ljchang/DataXGaia/data/galaxia_mock/training_set_500k.npz')\n",
    "data_val = np.load('/tigress/ljchang/DataXGaia/data/galaxia_mock/validation_set_500k.npz')\n",
    "data_test = np.load('/tigress/ljchang/DataXGaia/data/galaxia_mock/test_set_500k.npz')\n",
    "print(data_train.files)\n",
    "print(data_train['data'].shape)\n",
    "data_train = data_train['data']\n",
    "data_val = data_val['data']\n",
    "data_test = data_test['data']\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = ['source_id', 'l', 'b', 'ra', 'dec', 'parallax', 'parallax_error', \n",
    "             'pmra', 'pmra_error', 'pmdec', 'pmdec_error', 'radial_velocity',\n",
    "             'photo_g_mean_mag', 'photo_bp_mean_mag', 'photo_rp_mean_mag',\n",
    "             'x','y','z','vx','vy','vz','r','phi','theta','vr','vphi','vtheta']\n",
    "#could train on radial_velocity_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.DataFrame(data, columns=data_cols)\n",
    "#data = data.drop(data[data.parallax < 2].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.DataFrame(data_train, columns=data_cols)\n",
    "data_val = pd.DataFrame(data_val, columns=data_cols)\n",
    "data_test = pd.DataFrame(data_test, columns=data_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing radial velocity column with nonrescaled radial velocity \n",
    "#so no inverse_transform needed at the end\n",
    "#data_train_scaled = data_train_scaled.assign(radial_velocity=data_train['radial_velocity'])\n",
    "#data_val_scaled = data_val_scaled.assign(radial_velocity=data_val['radial_velocity'])\n",
    "#data_test_scaled = data_test_scaled.assign(radial_velocity=data_test['radial_velocity'])\n",
    "l_train = (data_train['l']).values\n",
    "l_val = (data_val['l']).values\n",
    "\n",
    "vr_train = (data_train['radial_velocity']).values\n",
    "vr_val = (data_val['radial_velocity']).values\n",
    "#print(np.min(vr_real_train))\n",
    "#print(np.max(vr_real_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Laura's weights \n",
    "from scipy.interpolate import interp2d\n",
    "from tqdm import tqdm\n",
    "weight_type = \"log2d\"\n",
    "lbins2d = np.linspace(0,360,51)\n",
    "vbins2d = np.linspace(-550,550,51)\n",
    "\n",
    "lbins2d_centers = (lbins2d[1:]+lbins2d[:-1])/2\n",
    "vbins2d_centers = (vbins2d[1:]+vbins2d[:-1])/2\n",
    "\n",
    "counts2d_train = np.histogram2d(l_train,vr_train,bins=[lbins2d,vbins2d])[0]\n",
    "counts2d_val = np.histogram2d(l_val,vr_val,bins=[lbins2d,vbins2d])[0]\n",
    "\n",
    "if weight_type == \"lin2d\":\n",
    "    invweight_func_train = interp2d(lbins2d_centers,vbins2d_centers,counts2d_train.T)\n",
    "    invweight_func_val = interp2d(lbins2d_centers,vbins2d_centers,counts2d_val.T)\n",
    "\n",
    "    invweights2d_train = np.zeros(len(l_train))\n",
    "    invweights2d_val = np.zeros(len(l_val))\n",
    "\n",
    "    for i in tqdm(range(len(l_train))):\n",
    "        invweights2d_train[i] = invweight_func_train(l_train[i],vr_train[i])\n",
    "    for j in tqdm(range(len(l_val))):\n",
    "        invweights2d_val[j] = invweight_func_val(l_val[j],vr_val[j])\n",
    "\n",
    "    weights_train = 1/invweights2d_train\n",
    "\n",
    "    weights_val = 1/invweights2d_val\n",
    "\n",
    "    print(\"Using linear weights in vr and l\")\n",
    "\n",
    "elif weight_type == \"log2d\":\n",
    "    invweight_func_train = interp2d(lbins2d_centers,vbins2d_centers,counts2d_train.T)\n",
    "    invweight_func_val = interp2d(lbins2d_centers,vbins2d_centers,counts2d_val.T)\n",
    "\n",
    "    invweights2d_train = np.zeros(len(l_train))\n",
    "    invweights2d_val = np.zeros(len(l_val))\n",
    "\n",
    "    for i in tqdm(range(len(l_train))):\n",
    "        invweights2d_train[i] = invweight_func_train(l_train[i],vr_train[i])\n",
    "    for j in tqdm(range(len(l_val))):\n",
    "        invweights2d_val[j] = invweight_func_val(l_val[j],vr_val[j])\n",
    "\n",
    "    weights_train = 1/invweights2d_train\n",
    "    weights_train = np.log(weights_train)\n",
    "    weights_train = weights_train - np.min(weights_train) + 1\n",
    "\n",
    "    weights_val = 1/invweights2d_val\n",
    "    weights_val = np.log(weights_val)\n",
    "    weights_val = weights_val - np.min(weights_val) + 1\n",
    "\n",
    "    print(\"Using log weights in vr and l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the training weights\n",
    "print(np.min(vr_train))\n",
    "print(np.max(vr_train))\n",
    "counts_train, bins_train = np.histogram(vr_train,bins=np.linspace(-700,700,51))\n",
    "#plt.hist(vr_real_train, bins=np.linspace(-10,400,51))\n",
    "bin_centers_train = (bins_train[1:]+bins_train[:-1])/2\n",
    "#print(bin_centers_train)\n",
    "interp_func_train  = interp1d(bin_centers_train,(counts_train).astype('float'))\n",
    "inv_weights_train = interp_func_train(vr_train)\n",
    "weights_train = 1/inv_weights_train\n",
    "weights_train = np.sqrt(weights_train)\n",
    "#weights_train = np.log(weights_train)\n",
    "#weights_train = weights_train - np.min(weights_train)+1\n",
    "plt.figure()\n",
    "\n",
    "plt.hist(weights_train)\n",
    "print(np.max(weights_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the training weights\n",
    "counts_val, bins_val = np.histogram(vr_val,bins=np.linspace(-700,700,100))\n",
    "bin_centers_val = (bins_val[1:]+bins_val[:-1])/2\n",
    "interp_func_val  = interp1d(bin_centers_val,(counts_val).astype('float'))\n",
    "inv_weights_val = interp_func_val(vr_val)\n",
    "weights_val = 1/inv_weights_val\n",
    "weights_val = np.sqrt(weights_val)\n",
    "#weights_val = np.log(weights_val)\n",
    "#weights_val = weights_val - np.min(weights_val)+1\n",
    "\n",
    "print(np.max(weights_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a test_weights array so I don't have to define separate class for TestLoader\n",
    "weights_test = np.ones(data_test.shape[0])\n",
    "#weights_train_2 = np.ones(data_train.shape[0])\n",
    "#weights_val_2 = np.ones(data_val.shape[0])\n",
    "print(np.shape(weights_test))\n",
    "print(weights_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['l'] = data_train['l'].apply(lambda x: np.cos(x))\n",
    "#data_train['b'] = data_train['b'].apply(lambda x: np.cos(x))\n",
    "#data_train['pmra'] = data_train['pmra'].apply(lambda x: np.cos(x))\n",
    "#data_train['pmdec'] = data_train['pmdec'].apply(lambda x: np.cos(x))\n",
    "\n",
    "data_val['l'] = data_val['l'].apply(lambda x: np.cos(x))\n",
    "#data_val['b'] = data_val['b'].apply(lambda x: np.cos(x))\n",
    "#data_val['pmra'] = data_val['pmra'].apply(lambda x: np.cos(x))\n",
    "#data_val['pmdec'] = data_val['pmdec'].apply(lambda x: np.cos(x))\n",
    "\n",
    "data_test['l'] = data_test['l'].apply(lambda x: np.cos(x))\n",
    "#data_test['b'] = data_test['b'].apply(lambda x: np.cos(x))\n",
    "#data_test['pmra'] = data_test['pmra'].apply(lambda x: np.cos(x))\n",
    "#data_test['pmdec'] = data_test['pmdec'].apply(lambda x: np.cos(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS = StandardScaler()\n",
    "mu = np.mean((data_train['radial_velocity']).values)\n",
    "stddev = np.std((data_train['radial_velocity']).values)\n",
    "data_train_scaled = SS.fit_transform(data_train)\n",
    "data_val_scaled = SS.transform(data_val)\n",
    "data_test_scaled = SS.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_scaled = pd.DataFrame(data_train_scaled, columns=data_cols)\n",
    "data_val_scaled = pd.DataFrame(data_val_scaled, columns=data_cols)\n",
    "data_test_scaled = pd.DataFrame(data_test_scaled, columns=data_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = ['l', 'b','parallax','pmra','pmdec']\n",
    "# Make the design matrix\n",
    "X_train = data_train_scaled[use_cols].values\n",
    "y_train = (data_train_scaled['radial_velocity']).values\n",
    "\n",
    "X_val = data_val_scaled[use_cols].values\n",
    "y_val = (data_val_scaled['radial_velocity']).values\n",
    "\n",
    "X_test = data_test_scaled[use_cols].values\n",
    "y_test = (data_test_scaled['radial_velocity']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import callbacks as callbacks\n",
    "global index \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LikelihoodLossFunction(y_true, y_pred):\n",
    "    # shape of y_pred should be (nsamples, 2)\n",
    "    # the first column should be the mean of the prediction\n",
    "    # the second column is the confidence (number of standard deviations)\n",
    "#     print y_true.shape\n",
    "#     print y_pred.shape\n",
    "    SIGMA = K.abs(y_pred[:, 1]) + 1e-6\n",
    "\n",
    "    LOC = y_pred[:, 0]\n",
    "    \n",
    "    X = y_true[:, 0]\n",
    "    weights = y_true[:,1]\n",
    "    ARG = K.abs(X - LOC) / (2 * K.abs(SIGMA))\n",
    "    PREFACT = K.log(K.pow(2 * np.pi * K.square(SIGMA), -0.5))\n",
    "    return K.mean((ARG - PREFACT) * weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConstantLikelihoodLossFunction(y_true, y_pred):\n",
    "    # shape of y_pred should be (nsamples, 2)\n",
    "    # the first column should be the mean of the prediction\n",
    "    # the second column is the confidence (number of standard deviations)\n",
    "#     print y_pred.shape\n",
    "    LOC = y_pred[:,0]\n",
    "    X = y_true[:, 0]\n",
    "    weights = y_true[:,1]\n",
    "    ARG = K.square(X - LOC) / (2.0)\n",
    "    PREFACT = K.log(K.pow(2 * np.pi, -0.5))\n",
    "    return K.mean((ARG - PREFACT) * weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two network technique to calculate the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Input, Dense, Lambda, Concatenate, Dropout, Activation, Add\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import clear_output\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_low = -5\n",
    "y_high = 5\n",
    "bin_num = 100\n",
    "plt.hist((data_train_scaled['radial_velocity']).values, bins=bin_num, range=(y_low,y_high), histtype='step', edgecolor = 'purple', color= 'skyblue', label = 'vlos', alpha=0.5, density = True)\n",
    "plt.hist((data_train_scaled['radial_velocity']).values, weights = weights_train, bins=bin_num, range=(y_low,y_high), histtype='step', color= 'orange', label = 'weighted', density = True)\n",
    "plt.hist(-(data_train_scaled['radial_velocity']).values, bins=bin_num, range=(y_low,y_high), histtype='step', edgecolor = 'magenta', color= 'pink',  label = '-vlos', alpha = 0.5, density = True)\n",
    "plt.hist(-(data_train_scaled['radial_velocity']).values, weights = weights_train, bins=bin_num, range=(y_low,y_high), histtype='step', color= 'red', label = 'weighted', density = True)\n",
    "#plt.hist(y[:,0], bins=bin_num, range=(y_low,y_high), histtype='step', color= 'blue',  label = 'unweighted after loader', density = True)\n",
    "#plt.hist(y[:,0], bins=bin_num, weights = y[:,1], range=(y_low,y_high), histtype='step', color= 'red', label = 'weighed after loader', density = True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.title('train')\n",
    "print(np.max(weights_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.hstack([y_train, weights_train])\n",
    "y_val = np.hstack([y_val, weights_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(inputs, MeanEst, MeanModel, ConfEst, ConfModel)\n",
    "inputs = Input(shape=(5,))\n",
    "MeanEst = (Dense(30, activation='tanh'))(inputs)\n",
    "MeanEst = (Dropout(0.1))(MeanEst)\n",
    "MeanEst = (Dense(30, activation='tanh'))(MeanEst)\n",
    "MeanEst = (Dropout(0.1))(MeanEst)\n",
    "MeanEst = (Dense(30, activation='tanh'))(MeanEst)\n",
    "MeanEst = (Dropout(0.1))(MeanEst)\n",
    "MeanEst = (Dense(30, activation='tanh'))(MeanEst)\n",
    "MeanEst = (Dropout(0.1))(MeanEst)\n",
    "MeanEst = (Dense(1, activation='linear'))(MeanEst)\n",
    "MeanModel = Model(inputs=[inputs], outputs=MeanEst)\n",
    "\n",
    "ConfEst= (Dense(30, activation='tanh'))(inputs)\n",
    "ConfEst = (Dropout(0.1))(ConfEst)\n",
    "ConfEst= (Dense(30, activation='tanh'))(ConfEst)\n",
    "ConfEst = (Dropout(0.1))(ConfEst)\n",
    "ConfEst= (Dense(30, activation='tanh'))(ConfEst)\n",
    "ConfEst = (Dropout(0.1))(ConfEst)\n",
    "ConfEst= (Dense(30, activation='tanh'))(ConfEst)\n",
    "ConfEst = (Dropout(0.1))(ConfEst)\n",
    "ConfEst= (Dense(1, activation='relu'))(ConfEst)\n",
    "ConfModel = Model(inputs=[inputs], outputs=ConfEst)\n",
    "#how can this give me a confidence output? the only thing that appears to define a confidence is the relu\n",
    "#on the last layer...think about this.\n",
    "#CombinedSub = Concatenate(axis=-1)([MeanEst, ConfEst])\n",
    "CombinedSub = Concatenate(axis=-1)([MeanModel(inputs), ConfModel(inputs)])\n",
    "\n",
    "CombinedModel = Model(inputs=[inputs], outputs=CombinedSub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CombinedModel.save_weights('modelsIterate/TrainingBoth_0000.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CombinedModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPoint = ModelCheckpoint('models_GALAXIA_nonn/TrainingMean_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR = ReduceLROnPlateau(patience=10, min_lr=1e-5, verbose=True)\n",
    "CSV_logger = CSVLogger(filename='logs/training_mean.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks = [CheckPoint, ES, RLR, CSV_logger]\n",
    "#mycallbacks = [ES, RLR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfModel.trainable = False\n",
    "MeanModel.trainable = True\n",
    "CombinedModel.compile(loss=ConstantLikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Model (Constant Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPoint2 = ModelCheckpoint('models_GALAXIA_nonn/TrainingErrorBars_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES2 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR2 = ReduceLROnPlateau(patience=10, min_lr=1e-5)\n",
    "CSV_logger2 = CSVLogger(filename='logs/training_errorbars.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks2 = [CheckPoint2, ES2, RLR2, CSV_logger2]\n",
    "#mycallbacks2 = [ES2, RLR2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfModel.trainable = True\n",
    "MeanModel.trainable = False\n",
    "CombinedModel.compile(loss=LikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks2\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Confidence Model (Likelihood Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPoint4 = ModelCheckpoint('models_GALAXIA_nonn/TrainingMean2_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES4 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR4 = ReduceLROnPlateau(patience=10, min_lr=1e-5, verbose=True)\n",
    "CSV_logger4 = CSVLogger(filename='logs/training_mean2.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks4 = [CheckPoint4, ES4, RLR4, CSV_logger4]\n",
    "#mycallbacks4 = [ES4, RLR4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfModel.trainable = False\n",
    "MeanModel.trainable = True\n",
    "CombinedModel.compile(loss=ConstantLikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks4\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Model (2nd Train) (Constant Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPoint5 = ModelCheckpoint('models_GALAXIA_nonn/TrainingErrorBars2_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES5 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR5 = ReduceLROnPlateau(patience=10, min_lr=1e-5)\n",
    "CSV_logger5 = CSVLogger(filename='logs/training_errorbars2.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks5 = [CheckPoint5, ES5, RLR5, CSV_logger5]\n",
    "#mycallbacks5 = [ES5, RLR5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfModel.trainable = True\n",
    "MeanModel.trainable = False\n",
    "CombinedModel.compile(loss=LikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks5\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Confidence Model (2nd Train) (Likelihood Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "CheckPoint6 = ModelCheckpoint('models_GALAXIA_nonn/TrainingMean3_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES4 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR4 = ReduceLROnPlateau(patience=10, min_lr=1e-5, verbose=True)\n",
    "CSV_logger6 = CSVLogger(filename='logs/training_mean3.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks6 = [CheckPoint6, ES4, RLR4, CSV_logger6]\n",
    "#mycallbacks4 = [ES4, RLR4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ConfModel.trainable = False\n",
    "MeanModel.trainable = True\n",
    "CombinedModel.compile(loss=ConstantLikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks6\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Model (3rd Train) (Constant Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "CheckPoint7 = ModelCheckpoint('models_GALAXIA_nonn/TrainingErrorBars3_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES5 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR5 = ReduceLROnPlateau(patience=10, min_lr=1e-5)\n",
    "CSV_logger7 = CSVLogger(filename='logs/training_errorbars3.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks7 = [CheckPoint7, ES5, RLR5, CSV_logger7]\n",
    "#mycallbacks5 = [ES5, RLR5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ConfModel.trainable = True\n",
    "MeanModel.trainable = False\n",
    "CombinedModel.compile(loss=LikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks7,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Confidence Model (3rd Train) (Likelihood Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPoint3 = ModelCheckpoint('models_GALAXIA_nonn/TrainingBoth_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES3 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR3 = ReduceLROnPlateau(patience=10, min_lr=1e-5)\n",
    "CSV_logger3 = CSVLogger(filename='logs/training_both.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks3 = [CheckPoint3, ES3, RLR3, CSV_logger3]\n",
    "#mycallbacks3 = [ES3, RLR3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfModel.trainable = True\n",
    "MeanModel.trainable = True\n",
    "CombinedModel.compile(loss=LikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks3\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Both Models (Likelihood Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.savefig('Loss_Plot_Iterate.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CombinedModel.save_weights(\"models_GALAXIA_nonn/ModelWeights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    " def plot_validation(i,type1):   \n",
    "    y_low = -700\n",
    "    y_high = 700\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    gs0 = gridspec.GridSpec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    from matplotlib.colors import LogNorm\n",
    "    plt.subplot(gs0[3])\n",
    "    plt.hist2d( test_preds[:,1],test_preds[:,0], bins=40,norm = LogNorm())\n",
    "    clb1 = plt.colorbar()\n",
    "    clb1.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel('$\\sigma$',labelpad=-10)\n",
    "\n",
    "    plt.subplot(gs0[4])\n",
    "    sc = plt.scatter(y_val[:,0], test_preds[:,0], c =test_preds[:,1], label = 'data',marker = '.',cmap=plt.cm.YlOrRd)\n",
    "    x1 = np.linspace(y_low,y_high,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.xlim([y_low, y_high])\n",
    "    plt.ylim([y_low,y_high])\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    clb2 = plt.colorbar(sc)\n",
    "    clb2.set_label('$\\sigma$', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.clim(0,750)\n",
    "\n",
    "    plt.subplot(gs0[2])\n",
    "    hb = plt.hexbin(y_val[:,0], test_preds[:,0],gridsize=80, norm = LogNorm())\n",
    "    x1 = np.linspace(np.min(y_val[:,0]),np.max(y_val[:,0]),1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    clb3 = plt.colorbar(hb)\n",
    "    clb3.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "\n",
    "\n",
    "    plt.subplot(gs0[5])\n",
    "    sc = plt.scatter(y_val[:,0], test_preds[:,0], c =test_preds[:,1], label = 'data',marker = '.',cmap=plt.cm.YlOrRd)\n",
    "    x1 = np.linspace(y_low,y_high,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.xlim([y_low, y_high])\n",
    "    plt.ylim([y_low,y_high])\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    clb4 = plt.colorbar(sc)\n",
    "    clb4.set_label('$\\sigma$\\n(saturated)', labelpad=-25, y=1.15, rotation=0,fontsize=10)\n",
    "    plt.clim(0,200)\n",
    "\n",
    "    plt.subplot(gs0[6])\n",
    "    plt.hist2d(X_val[:,2], test_preds[:,1], bins=40, norm = LogNorm())\n",
    "    clb5 = plt.colorbar()\n",
    "    clb5.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.xlabel('Distance',fontsize = 12)\n",
    "    plt.ylabel('$\\sigma$',labelpad=-5)\n",
    "\n",
    "    plt.subplot(gs0[1])\n",
    "    plotrange = np.linspace(-5,5,1000)\n",
    "    diff_hist = np.divide(np.subtract(test_preds[:,0],y_val[:,0]),test_preds[:,1])\n",
    "    mean_diffs, mean_stds = np.mean(diff_hist), np.std(diff_hist)\n",
    "    plt.hist(diff_hist,bins=20, range=(-5,5), histtype='step',color = 'lightseagreen',linewidth = 1.5, density = True)\n",
    "    plt.plot(plotrange, norm.pdf(plotrange, mean_diffs, mean_stds),color = 'darkorange', linestyle = '--', linewidth = 2.5,label = 'normal fit')\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 8})\n",
    "    plt.xlabel(r'$(v_{\\rm{los}}^{\\rm{pred}} - v_{\\rm{los}}^{\\rm{meas}})/\\sigma$',labelpad=-5)\n",
    "    \n",
    "    plt.subplot(gs0[7])\n",
    "    diff_hist_err = np.subtract(test_preds[:,0],y_val[:,0])\n",
    "    plt.hist2d(test_preds[:,1],diff_hist_err, bins=40,norm = LogNorm())\n",
    "    clb6 = plt.colorbar()\n",
    "    clb6.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$(v_{\\rm{los}}^{\\rm{pred}} - v_{\\rm{los}}^{\\rm{meas}})$',labelpad=-5)\n",
    "    plt.xlabel('$\\sigma$',labelpad=-5)\n",
    "\n",
    "    plt.subplot(gs0[0])\n",
    "    plt.hist(y_val[:,0], bins=25, range=(y_low,y_high), histtype='step', edgecolor = 'skyblue', color= 'skyblue', fill = True, label = 'val' )\n",
    "    plt.hist(test_preds[:,0], bins=25, range=(y_low,y_high), histtype='step',color = 'blue', label = 'predicted')\n",
    "    plt.xlabel(r'$v_{\\rm{los}}$', labelpad =-2)\n",
    "    plt.title('Validation set, Train '+type1+', Epoch '+str(i)+', 20 bins',fontsize=14)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 10})\n",
    "    \n",
    "    plt.savefig('plots'+type1+'/'+'train'+type1+'_'+str(i)+'.png')\n",
    "    clb1.remove()\n",
    "    clb2.remove()\n",
    "    clb3.remove()\n",
    "    clb4.remove()\n",
    "    clb5.remove()\n",
    "    clb6.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting First Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininmean = [file for file in os.listdir('models/')\n",
    "               if file.startswith('TrainingMean_')]\n",
    "traininmean = sorted(traininmean)[1::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i, mfile in enumerate(traininmean):\n",
    "    CombinedModel.load_weights('models/' + mfile)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i*10,'Mean1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininerror = [file for file in os.listdir('models/')\n",
    "               if file.startswith('TrainingErrorBars_')]\n",
    "traininerror = sorted(traininerror)[0::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i_err, mfile_err in enumerate(traininerror):\n",
    "    CombinedModel.load_weights('models/' + mfile_err)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i_err*20,'Error1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Second Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininmean2 = [file for file in os.listdir('models/')\n",
    "               if file.startswith('TrainingMean2')]\n",
    "traininmean2 = sorted(traininmean2)[0::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i, mfile in enumerate(traininmean2):\n",
    "    CombinedModel.load_weights('models/' + mfile)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i*10,'Mean2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininerror2 = [file for file in os.listdir('models/')\n",
    "               if file.startswith('TrainingErrorBars2')]\n",
    "traininerror2 = sorted(traininerror2)[0::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i_err, mfile_err in enumerate(traininerror2):\n",
    "    CombinedModel.load_weights('models/' + mfile_err)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i_err*20,'Error2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Third Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininmean3 = [file for file in os.listdir('models/')\n",
    "               if file.startswith('TrainingMean3')]\n",
    "traininmean3 = sorted(traininmean3)[0::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i, mfile in enumerate(traininmean3):\n",
    "    CombinedModel.load_weights('models/' + mfile)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i*10,'Mean3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininerror3 = [file for file in os.listdir('models/')\n",
    "               if file.startswith('TrainingErrorBars3')]\n",
    "traininerror3 = sorted(traininerror3)[0::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i_err, mfile_err in enumerate(traininerror3):\n",
    "    CombinedModel.load_weights('models/' + mfile_err)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i_err*20,'Error3');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results of training both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininboth = [file for file in os.listdir('models/')\n",
    "               if file.startswith('TrainingBoth')]\n",
    "traininboth = sorted(traininboth)[1::10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i_both, mfile_both in enumerate(traininboth):\n",
    "    CombinedModel.load_weights('models/' + mfile_both)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i_both*10,'Both');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CombinedModel.load_weights('G_train_2it_nn_500k_tanh_D30_p1dropout_sinl_sigmaleq_0/models_GALAXIA/' + 'ModelWeights.h5')\n",
    "test_preds_2 = CombinedModel.predict(X_test)\n",
    "y_low = -250\n",
    "y_high = 250\n",
    "#here is where I want to rescale test_preds_2[:,0]\n",
    "test_preds_2[:,0] = (test_preds_2[:,0] * stddev)+mu\n",
    "test_preds_2[:,1] = (test_preds_2[:,1] * stddev)+mu\n",
    "#does not work to set = 0 after the fact. \n",
    "#for elem_i in range(len(test_preds_2[:,0])):\n",
    "   # if ((test_preds_2[elem_i,0] >= -10) and (test_preds_2[elem_i,0] <= 10)):\n",
    "      #  test_preds_2[elem_i, 0] = 0\n",
    "\n",
    "print(test_preds_2[:,0].shape)\n",
    "high_err = test_preds_2[:,0] + test_preds_2[:,1]\n",
    "print(high_err.shape)\n",
    "low_err = test_preds_2[:,0] - test_preds_2[:,1]\n",
    "test_preds_hist = np.histogram(test_preds_2[:,0], bins=100, range=(y_low,y_high), density= True)\n",
    "test_err_hist = np.histogram(test_preds_2[:,1], bins=100, range=(y_low,y_high), density= True)\n",
    "high_err_hist = np.histogram(high_err, bins=100, range=(y_low,y_high), density= True)\n",
    "low_err_hist = np.histogram(low_err, bins=100, range=(y_low,y_high), density= True)\n",
    "conv1 = np.convolve(test_preds_hist[0], high_err_hist[0],'same')\n",
    "conv2 = np.convolve(high_err_hist[0], low_err_hist[0], 'same')\n",
    "conv3 = np.convolve(test_preds_hist[0], low_err_hist[0], 'same')\n",
    "conv4 = np.convolve(test_preds_hist[0],test_err_hist[0],'same')\n",
    "plt.hist((data_test['radial_velocity']).values, bins=100, range=(y_low,y_high), histtype='step', edgecolor = 'skyblue', color= 'skyblue', fill = True, density = True, label = 'test' , zorder = 0, alpha = 0.5)\n",
    "#plt.hist(low_err, bins=100, range=(y_low,y_high), density= True, alpha = 0.5)\n",
    "#plt.hist(high_err, bins=100, range=(y_low,y_high), density= True,alpha = 0.5)\n",
    "plt.hist(test_preds_2[:,0], bins=100, range=(y_low,y_high), density= True,alpha = 0.5, color = 'blue', label = 'predicted')\n",
    "#plt.hist(test_preds_2[:,1], bins=100, range=(0,100), density= True)\n",
    "#plt.figure()\n",
    "#plt.plot(conv2, label = 'conv2')\n",
    "#plt.plot(conv1,label = 'conv1')\n",
    "#plt.plot(conv3,label = 'conv3')\n",
    "#plt.plot(conv4,label = 'conv4')\n",
    "#perhpas conv 2 gives errors on histogram bins\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "print(conv1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist((data_test['radial_velocity']).values, bins=100, range=(y_low,y_high), histtype='step', color= 'blue', label = 'test', density = True, alpha = 0.5)\n",
    "num, binss, _ = plt.hist(test_preds_2[:,0], bins=100, range=(y_low,y_high), histtype='step',color = 'green',  label = 'predicted', density = True, alpha = 0.5)\n",
    "mid = 0.5*(binss[1:] + binss[:-1])\n",
    "print(mid.shape)\n",
    "#plt.errorbar(mid, num,yerr = conv2, fmt='.',color = 'blue', label = 'predicted')\n",
    "plt.fill_between(mid[0:50],num[0:50]-conv3[0:50],num[0:50]+conv3[0:50] ,label = 'error from conv',color = 'red', zorder = 10, alpha = 0.5)\n",
    "plt.fill_between(mid[49:100],num[49:100]-conv1[49:100],num[49:100]+conv1[49:100] ,color = 'red', zorder = 10, alpha = 0.5)\n",
    "\n",
    "#plt.fill_between(mid,num-conv1,num+conv1 ,label = 'error',color = 'red', zorder = 10, alpha = 0.5)\n",
    "#capsize=3\n",
    "plt.xlabel('Vr')\n",
    "plt.ylabel('Stars')\n",
    "plt.title('Test set - simulated data')\n",
    "#plt.yscale('log')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#going to consider low error points those with error <= 50 km/s\n",
    "#test_preds_70 = np.delete(test_preds_2, np.where(test_preds_2[:,1]>=70)[0], 0)\n",
    "#print(test_preds_70.shape)\n",
    "test_preds_50 = np.delete(test_preds_2, np.where(test_preds_2[:,1]>=50)[0], 0)\n",
    "print(test_preds_50.shape)\n",
    "test_preds_45 = np.delete(test_preds_2, np.where(test_preds_2[:,1]>=45)[0], 0)\n",
    "print(test_preds_45.shape)\n",
    "test_preds_40 = np.delete(test_preds_2, np.where(test_preds_2[:,1]>=40)[0], 0)\n",
    "print(test_preds_40.shape)\n",
    "test_preds_35 = np.delete(test_preds_2, np.where(test_preds_2[:,1]>=35)[0], 0)\n",
    "print(test_preds_35.shape)\n",
    "test_preds_25 = np.delete(test_preds_2, np.where(test_preds_2[:,1]>=25)[0], 0)\n",
    "print(test_preds_25.shape)\n",
    "test_preds_18 = np.delete(test_preds_2, np.where(test_preds_2[:,1]>=18)[0], 0)\n",
    "print(test_preds_18.shape)\n",
    "test_preds_22 = np.delete(test_preds_2, np.where(test_preds_2[:,1]>=22)[0], 0)\n",
    "print(test_preds_22.shape)\n",
    "test_preds_20 = np.delete(test_preds_2, np.where(test_preds_2[:,1]>=20)[0], 0)\n",
    "print(test_preds_20.shape)\n",
    "test_preds_30 = np.delete(test_preds_2, np.where(test_preds_2[:,1]>=30)[0], 0)\n",
    "print(test_preds_30.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_indices(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [22,25,30]\n",
    "for thresh_i in thresholds: \n",
    "    save_indices(thresh_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going to use Monte Carlo simulations to predict errorbars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo(df, test_preds_cut, thresh):  \n",
    "    from matplotlib.colors import LogNorm\n",
    "    mc_vr_pred_list = []\n",
    "    bin_values_list = []\n",
    "    min_array = []\n",
    "    max_array = []\n",
    "    min_array_r = []\n",
    "    max_array_r = []\n",
    "    min_array_th = []\n",
    "    max_array_th = []\n",
    "    min_array_phi = []\n",
    "    max_array_phi = []\n",
    "\n",
    "    data_test = reload_data_per_cut(thresh)\n",
    "    hb_list = []\n",
    "    hb_list_r = []\n",
    "    hb_list_th = []\n",
    "    hb_list_phi = []\n",
    "    hex_centers = []\n",
    "    \n",
    "    bin_values_list_r = []\n",
    "    bin_values_list_th = []\n",
    "    bin_values_list_phi = []\n",
    "    for mc_i in range(0,100):\n",
    "        mc_vr_pred = []\n",
    "        for star_i in range(0,len(test_preds_cut)):\n",
    "            #mc_vr_pred.append(np.mean([np.random.normal(test_preds_2[star_i,0],test_preds_2[star_i,1]) for _ in range(1000)]))\n",
    "            mc_vr_pred.append(np.random.normal(test_preds_cut[star_i,0],test_preds_cut[star_i,1]))\n",
    "        mc_vr_pred_list.append(mc_vr_pred)\n",
    "        n, bins = np.histogram(mc_vr_pred, bins=50, range=(y_low,y_high))\n",
    "        \n",
    "        plt.figure(2)\n",
    "        hb = plt.hexbin((data_test['radial_velocity']).values, mc_vr_pred,gridsize=100, norm = LogNorm(),extent=[-200, 200, -200, 200]);\n",
    "        plt.close(2)\n",
    "        hb_list.append(hb.get_array());\n",
    "        bin_values_list.append(n)\n",
    "        \n",
    "        #now for the coordinate-transformed histograms\n",
    "        vel_sph_coord = get_coord_transform(data_test, np.array(mc_vr_pred))\n",
    "        n_r , bins_r = np.histogram(vel_sph_coord[:,0], bins=50, range=(-250,250))\n",
    "        n_th , bins_th = np.histogram(vel_sph_coord[:,1], bins=50, range=(-250,250))\n",
    "        n_phi , bins_phi = np.histogram(vel_sph_coord[:,2], bins=50, range=(-450,0))\n",
    "        bin_values_list_r.append(n_r)\n",
    "        bin_values_list_th.append(n_th)\n",
    "        bin_values_list_phi.append(n_phi)\n",
    "        \n",
    "        plt.figure(3)\n",
    "        hb_r = plt.hexbin((data_test['vr']).values, vel_sph_coord[:,0],gridsize=100, norm = LogNorm(),extent=[-250, 250, -250, 250]);\n",
    "        plt.close(3)\n",
    "        hb_list_r.append(hb_r.get_array());\n",
    "        \n",
    "        plt.figure(4)\n",
    "        hb_th = plt.hexbin((data_test['vtheta']).values, vel_sph_coord[:,1],gridsize=100, norm = LogNorm(),extent=[-250, 250, -250, 250]);\n",
    "        plt.close(4)\n",
    "        hb_list_th.append(hb_th.get_array());\n",
    "        \n",
    "        plt.figure(5)\n",
    "        hb_phi = plt.hexbin((data_test['vphi']).values, vel_sph_coord[:,2],gridsize=100, norm = LogNorm(),extent=[-450, 0, -450, 0]);\n",
    "        plt.close(5)\n",
    "        hb_list_phi.append(hb_phi.get_array());\n",
    "        \n",
    "        \n",
    "    bin_values_list_arr = np.array(bin_values_list)    \n",
    "    max_array = bin_values_list_arr.max(axis=0)\n",
    "    min_array = bin_values_list_arr.min(axis=0) \n",
    "    \n",
    "    bin_values_list_r_arr = np.array(bin_values_list_r)    \n",
    "    max_array_r = bin_values_list_r_arr.max(axis=0)\n",
    "    min_array_r = bin_values_list_r_arr.min(axis=0) \n",
    "    \n",
    "    bin_values_list_th_arr = np.array(bin_values_list_th)    \n",
    "    max_array_th = bin_values_list_th_arr.max(axis=0)\n",
    "    min_array_th = bin_values_list_th_arr.min(axis=0) \n",
    "    \n",
    "    bin_values_list_phi_arr = np.array(bin_values_list_phi)    \n",
    "    max_array_phi = bin_values_list_phi_arr.max(axis=0)\n",
    "    min_array_phi = bin_values_list_phi_arr.min(axis=0) \n",
    "    \n",
    "\n",
    "    return min_array, max_array, hb_list, hb_list_r,hb_list_th,hb_list_phi, min_array_r,max_array_r,min_array_th,max_array_th, min_array_phi,max_array_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TransformCoords\n",
    "def get_coord_transform(df, train_preds):\n",
    "    #needs only vr values of train_preds (maybe...need to see what to do about error)\n",
    "    # m12i\n",
    "    # v_LSR = [224.7092,-20.3801, 3.8954]\n",
    "    # r_LSR = [0,8.2,0]\n",
    "    # Galaxia\n",
    "    v_LSR = [11.1, 239.08, 7.25]\n",
    "    r_LSR = [-8.,0.,0.015]\n",
    "    inds = np.arange(df.shape[0])\n",
    "    inds_train = np.arange(df.shape[0])\n",
    "    sub_num = df.shape[0]\n",
    "    vels_sph = np.array([df['vr'].values[inds][:sub_num],df['vtheta'].values[inds][:sub_num],df['vphi'].values[inds][:sub_num]]).T\n",
    "    coords_cart = np.array([df['x'].values[inds][:sub_num],df['y'].values[inds][:sub_num],df['z'].values[inds][:sub_num]]).T\n",
    "    \n",
    "    ra_cut = df['ra'].values[inds][:sub_num]\n",
    "    dec_cut = df['dec'].values[inds][:sub_num]\n",
    "    parallax_cut = df['parallax'].values[inds][:sub_num]\n",
    "    pmra_cut = df['pmra'].values[inds][:sub_num]\n",
    "    pmdec_cut = df['pmdec'].values[inds][:sub_num]\n",
    "    rv_cut = df['radial_velocity'].values[inds][:sub_num]\n",
    "    \n",
    "    U_pred_train,V_pred_train,W_pred_train = TransformCoords.pm2galcart(np.deg2rad(ra_cut[inds_train]),np.deg2rad(dec_cut[inds_train]),parallax_cut[inds_train],pmra_cut[inds_train],pmdec_cut[inds_train],train_preds.flatten().astype('float'))\n",
    "    \n",
    "    coords_cart_train = coords_cart[inds_train,:]\n",
    "    \n",
    "    vels_cart_pred_train = np.array([U_pred_train+v_LSR[0],V_pred_train+v_LSR[1],W_pred_train+v_LSR[2]]).T\n",
    "    \n",
    "    coords_sph_train, vels_sph_pred_train = TransformCoords.rvcart2sph_vec(coords_cart_train,vels_cart_pred_train)\n",
    "    \n",
    "    coords_sph_train[:,[1,2]] = coords_sph_train[:,[2,1]] # Swap theta, phi into correct order\n",
    "    \n",
    "    vels_sph_pred_train[:,[1,2]] = vels_sph_pred_train[:,[2,1]] # Swap theta, phi into correct order\n",
    "    \n",
    "    return vels_sph_pred_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_data_per_cut(thresh):\n",
    "    hold = 0\n",
    "    data_test = np.load('/tigress/ljchang/DataXGaia/data/galaxia_mock/test_set_500k.npz')\n",
    "    data_test = data_test['data']\n",
    "    data_cols = ['source_id', 'l', 'b', 'ra', 'dec', 'parallax', 'parallax_error', \n",
    "                 'pmra', 'pmra_error', 'pmdec', 'pmdec_error', 'radial_velocity',\n",
    "                 'photo_g_mean_mag', 'photo_bp_mean_mag', 'photo_rp_mean_mag',\n",
    "                 'x','y','z','vx','vy','vz','r','phi','theta','vr','vphi','vtheta']\n",
    "    data_test = pd.DataFrame(data_test, columns=data_cols)\n",
    "    print(data_test.shape)\n",
    "    indices_to_drop = []\n",
    "    if thresh >= 80:\n",
    "        indices_to_drop = np.load('data_indices_error_gt_'+str(thresh)+'.npy')\n",
    "        print(np.shape(indices_to_drop))\n",
    "        print('number of indices left '+ str(500000-np.shape(indices_to_drop)[0]))\n",
    "        print(indices_to_drop[332788:332798])\n",
    "        indices_to_drop = np.array(indices_to_drop)\n",
    "        print(indices_to_drop[0:1000])\n",
    "        print([item for item, count in collections.Counter(indices_to_drop).items() if count > 1])\n",
    "        #data_test = data_test.loc[indices_to_drop]\n",
    "        data_test = data_test.drop(data_test.index[indices_to_drop])\n",
    "        print(data_test.shape)\n",
    "    elif thresh < 80 and thresh > 0:\n",
    "        indices_to_drop = np.load('data_indices_error_lt_'+str(thresh)+'.npy')\n",
    "        print(np.shape(indices_to_drop))\n",
    "        print(indices_to_drop[1:10])\n",
    "        data_test = data_test.loc[indices_to_drop]\n",
    "        #data_test = data_test.drop(data_test.index[indices_to_drop])\n",
    "        print('second if '+str(data_test.shape))\n",
    "    elif thresh == 0: \n",
    "        hold = 0\n",
    "        print('hold = 0')\n",
    "\n",
    "    #pdiff_test = float(len(np.argwhere((data_test['radial_velocity']).values < 0))/len(np.argwhere((data_test['radial_velocity']).values > 0)))\n",
    "    #if pdiff_test > 1: data_test.drop(data_test.query('radial_velocity < 0').sample(frac=(pdiff_test-1)).index, inplace = True)\n",
    "    #pdiffafter_test = float(len(np.argwhere((data_test['radial_velocity']).values < 0))/len(np.argwhere((data_test['radial_velocity']).values > 0)))\n",
    "    weights_test = np.ones(data_test.shape[0])\n",
    "    return data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def plot_test(thresh): \n",
    "    data_test = reload_data_per_cut(thresh)\n",
    "    print('shape of data_test is '+str(data_test.shape))\n",
    "    if thresh == 0: test_preds = test_preds_2\n",
    "    elif thresh == 50: test_preds = test_preds_50\n",
    "    elif thresh == 45: test_preds = test_preds_45\n",
    "    elif thresh == 40: test_preds = test_preds_40\n",
    "    elif thresh == 30: test_preds = test_preds_30\n",
    "    elif thresh == 35: test_preds = test_preds_35\n",
    "    elif thresh == 25: test_preds = test_preds_25\n",
    "    elif thresh == 20: test_preds = test_preds_20\n",
    "    elif thresh == 22: test_preds = test_preds_22\n",
    "    elif thresh == 18: test_preds = test_preds_18\n",
    "    y_low = -250\n",
    "    y_high = 250\n",
    "    plt.figure(figsize=(12, 24))\n",
    "    gs0 = gridspec.GridSpec(6, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    from matplotlib.colors import LogNorm\n",
    "    plt.subplot(gs0[3])\n",
    "    plt.hist2d(test_preds[:,1],test_preds[:,0], bins=40,norm = LogNorm())\n",
    "    clb1 = plt.colorbar()\n",
    "    clb1.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel('$\\sigma$',labelpad=-5)\n",
    "    \n",
    "\n",
    "    plt.subplot(gs0[2])\n",
    "    hb = plt.hexbin((data_test['radial_velocity']).values, test_preds[:,0],gridsize=100, norm = LogNorm(),extent=[-200, 200, -200, 200])\n",
    "    #x1 = np.linspace(np.min((data_test_scaled['radial_velocity']).values),np.max((data_test_scaled['radial_velocity']).values),1000)\n",
    "    x1 = np.linspace(-150,150,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    clb3 = plt.colorbar(hb)\n",
    "    clb3.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "\n",
    "\n",
    "    plt.subplot(gs0[5])\n",
    "    plt.hist2d((data_test['l']).values,test_preds[:,0], bins=40,norm = LogNorm())\n",
    "    clb4 = plt.colorbar()\n",
    "    clb4.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.xlabel(r'$l$',labelpad=-3)\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-5)\n",
    "\n",
    "\n",
    "    plt.subplot(gs0[6])\n",
    "    dist_hist = np.divide(np.ones_like((data_test['parallax']).values),(data_test['parallax']).values)\n",
    "    plt.hist2d(dist_hist, test_preds[:,1], bins=40, norm = LogNorm())\n",
    "    clb5 = plt.colorbar()\n",
    "    clb5.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.xlabel('Distance (kpc)',fontsize = 12)\n",
    "    plt.ylabel('$\\sigma$',labelpad=-5)\n",
    "\n",
    "    plt.subplot(gs0[1])\n",
    "    plotrange = np.linspace(-5,5,1000)\n",
    "    diff_hist = np.divide(np.subtract(test_preds[:,0],(data_test['radial_velocity']).values),test_preds[:,1])\n",
    "    mean_diffs, mean_stds = np.mean(diff_hist), np.std(diff_hist)\n",
    "    plt.hist(diff_hist,bins=20, range=(-5,5), histtype='bar',ec = 'white', color = 'tab:blue',alpha = 0.5,fill = True, density = True)\n",
    "    plt.plot(plotrange, norm.pdf(plotrange, mean_diffs, mean_stds),color = 'darkorange', linestyle = '--', linewidth = 2.5,label = 'normal fit')\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 8})\n",
    "    plt.xlabel(r'$(v_{\\rm{los}}^{\\rm{pred}} - v_{\\rm{los}}^{\\rm{meas}})/\\sigma$',labelpad=-5)\n",
    "    \n",
    "    plt.subplot(gs0[4])\n",
    "    plt.hist2d((data_test['l']).values,test_preds[:,1], bins=40,norm = LogNorm())\n",
    "    clb2 = plt.colorbar()\n",
    "    clb2.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.xlabel(r'$l$',labelpad=-3)\n",
    "    plt.ylabel('$\\sigma$',labelpad=-5)\n",
    "    \n",
    "\n",
    "    plt.subplot(gs0[0])\n",
    "    plt.hist((data_test['radial_velocity']).values, bins=50, range=(y_low,y_high), histtype='bar', edgecolor = 'white', color= 'tab:blue',alpha = 0.5, fill = True, label = 'test', density = True )\n",
    "    plt.hist(test_preds[:,0], bins=100, range=(y_low,y_high), histtype='step',color = 'darkslateblue', linewidth = 1.3, label = 'predicted', density = True)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}$', labelpad =-2)\n",
    "    if thresh == 0: plt.title('Test set, 100 bins, no sigma cut',fontsize=14)\n",
    "    if thresh != 0: plt.title('Test set, $\\sigma \\leq$'+str(thresh)+' km/s',fontsize=14)\n",
    "    #plt.yscale('log')\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 10})\n",
    "    \n",
    "    \n",
    "    plt.subplot(gs0[7])\n",
    "    hist_test, bins_test, patches_test = plt.hist((data_test['radial_velocity']).values, bins=50, range=(y_low,y_high), histtype='bar', edgecolor = 'white', color= 'tab:blue', alpha = 0.5, fill = True, label = 'test' , zorder = 0)\n",
    "    bin_centers_test = (bins_test[1:]+bins_test[:-1])/2\n",
    "    min_array, max_array, hb_list, hb_list_r,hb_list_th,hb_list_phi, min_array_r,max_array_r,min_array_th,max_array_th, min_array_phi,max_array_phi = monte_carlo(data_test, test_preds, thresh);\n",
    "    plt.fill_between(bin_centers_test,min_array, max_array,label = 'MC spread',color = 'orange', zorder = 10, alpha = 0.5)\n",
    "    plt.hist(test_preds[:,0], bins=50, range=(y_low,y_high), histtype='step',color = 'darkslateblue',linewidth = 1.3, label = 'predicted', zorder = 20)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}$')\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 10})\n",
    "    \n",
    "    plt.subplot(gs0[8])\n",
    "    hb_mean=plt.hexbin((data_test['radial_velocity']).values, np.zeros_like((data_test['radial_velocity']).values), gridsize=100, norm = LogNorm(),extent=[-200, 200, -200, 200])\n",
    "    hb_mean.set_array(np.mean(hb_list, axis = 0))\n",
    "    x1 = np.linspace(-150,150,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred, MC}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    plt.clim(1,(np.max(hb_list)/2)*1.5)\n",
    "    clb6 = plt.colorbar(hb_mean)\n",
    "    clb6.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    \n",
    "    vels_sph_pred_test = get_coord_transform(data_test, test_preds)\n",
    "    #'vr','vphi','vtheta'\n",
    "    plt.subplot(gs0[9])\n",
    "    hb_r = plt.hexbin((data_test['vr']).values, vels_sph_pred_test[:,0],gridsize=100, norm = LogNorm(),extent=[-250, 250, -250, 250])\n",
    "    x1 = np.linspace(-250,250,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{r}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{r}}^{\\rm{meas}}$')\n",
    "    clb7 = plt.colorbar(hb_r)\n",
    "    clb7.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    \n",
    "    plt.subplot(gs0[10])\n",
    "    hb_t = plt.hexbin((data_test['vtheta']).values, vels_sph_pred_test[:,1],gridsize=100, norm = LogNorm(),extent=[-250, 250, -250, 250])\n",
    "    x2 = np.linspace(-250,250,1000)\n",
    "    y2 = x2\n",
    "    plt.plot(x2,y2,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{\\theta}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{\\theta}}^{\\rm{meas}}$')\n",
    "    clb8 = plt.colorbar(hb_t)\n",
    "    clb8.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    \n",
    "    plt.subplot(gs0[11])\n",
    "    hb_p = plt.hexbin((data_test['vphi']).values, vels_sph_pred_test[:,2],gridsize=100, norm = LogNorm(),extent=[-450, 0, -450, 0])\n",
    "    x3 = np.linspace(-450,0,1000)\n",
    "    y3 = x3\n",
    "    plt.plot(x3,y3,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{\\phi}}^{\\rm{pred}}$',labelpad=-5)\n",
    "    plt.xlabel(r'$v_{\\rm{\\phi}}^{\\rm{meas}}$')\n",
    "    clb9 = plt.colorbar(hb_p)\n",
    "    clb9.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    \n",
    "    plt.subplot(gs0[12])\n",
    "    hist_test_r, bins_test_r, patches_test_r = plt.hist((data_test['vr']).values, bins=50, range=(y_low,y_high), histtype='bar', edgecolor = 'white', color= 'tab:blue',alpha = 0.5, fill = True,  label = 'test' )\n",
    "    bin_centers_test_r = (bins_test_r[1:]+bins_test_r[:-1])/2\n",
    "    plt.fill_between(bin_centers_test_r,min_array_r, max_array_r,label = 'MC spread',color = 'orange',alpha = 0.5)\n",
    "    plt.hist(vels_sph_pred_test[:,0], bins=50, range=(y_low,y_high), histtype='step',color = 'darkslateblue', linewidth = 1.3,label = 'predicted')\n",
    "    plt.xlabel(r'$v_{\\rm{r}}$', labelpad =-2)\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 10})\n",
    "    \n",
    "    plt.subplot(gs0[13])\n",
    "    hist_test_th, bins_test_th, patches_test_th = plt.hist((data_test['vtheta']).values, bins=50, range=(y_low,y_high), histtype='bar', edgecolor = 'white', color= 'tab:blue',alpha = 0.5,fill = True, label = 'test', zorder = 0)\n",
    "    bin_centers_test_th = (bins_test_th[1:]+bins_test_th[:-1])/2\n",
    "    plt.fill_between(bin_centers_test_th,min_array_th, max_array_th,label = 'MC spread',color = 'orange', alpha = 0.5, zorder = 10)\n",
    "    plt.hist(vels_sph_pred_test[:,1], bins=50, range=(y_low,y_high), histtype='step',color = 'darkslateblue',linewidth = 1.3, label = 'predicted', zorder = 20)\n",
    "    plt.xlabel(r'$v_{\\rm{\\theta}}$', labelpad =-2)\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 10})\n",
    "    \n",
    "    plt.subplot(gs0[14])\n",
    "    hist_test_phi, bins_test_phi, patches_test_phi = plt.hist((data_test['vphi']).values, bins=50, range=(-450,0), histtype='bar', edgecolor = 'white', color= 'tab:blue', alpha = 0.5,fill = True,  label = 'test', zorder = 0)\n",
    "    bin_centers_test_phi = (bins_test_phi[1:]+bins_test_phi[:-1])/2\n",
    "    plt.fill_between(bin_centers_test_phi,min_array_phi, max_array_phi,label = 'MC spread',color = 'orange', alpha = 0.5, zorder = 10)\n",
    "    plt.hist(vels_sph_pred_test[:,2], bins=50, range=(-450,0), histtype='step',color = 'darkslateblue', linewidth = 1.3,label = 'predicted', zorder = 20)\n",
    "    plt.xlabel(r'$v_{\\rm{\\phi}}$', labelpad =-2)\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 10})\n",
    "    \n",
    "    plt.subplot(gs0[15])\n",
    "    hb_mean_r=plt.hexbin((data_test['vr']).values, np.zeros_like((data_test['vr']).values), gridsize=100, norm = LogNorm(),extent=[-250, 250, -250, 250])\n",
    "    hb_mean_r.set_array(np.mean(hb_list_r, axis = 0))\n",
    "    x1 = np.linspace(-250,250,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{r}}^{\\rm{pred, MC}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{r}}^{\\rm{meas}}$')\n",
    "    plt.clim(1,(np.max(hb_list_r)/2)*1.5)\n",
    "    clb10 = plt.colorbar(hb_mean_r)\n",
    "    clb10.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    \n",
    "    plt.subplot(gs0[16])\n",
    "    hb_mean_th=plt.hexbin((data_test['vtheta']).values, np.zeros_like((data_test['vtheta']).values), gridsize=100, norm = LogNorm(),extent=[-250, 250, -250, 250])\n",
    "    hb_mean_th.set_array(np.mean(hb_list_th, axis = 0))\n",
    "    x1 = np.linspace(-250,250,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{\\theta}}^{\\rm{pred, MC}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{\\theta}}^{\\rm{meas}}$')\n",
    "    plt.clim(1,(np.max(hb_list_th)/2)*1.5)\n",
    "    clb11 = plt.colorbar(hb_mean_th)\n",
    "    clb11.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    \n",
    "    plt.subplot(gs0[17])\n",
    "    hb_mean_phi=plt.hexbin((data_test['vphi']).values, np.zeros_like((data_test['vphi']).values), gridsize=100, norm = LogNorm(),extent=[-450, 0, -450, 0])\n",
    "    hb_mean_phi.set_array(np.mean(hb_list_phi, axis = 0))\n",
    "    x1 = np.linspace(-450,0,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{\\phi}}^{\\rm{pred, MC}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{\\phi}}^{\\rm{meas}}$')\n",
    "    plt.clim(1,(np.max(hb_list_phi)/2)*1.5)\n",
    "    clb12 = plt.colorbar(hb_mean_phi)\n",
    "    clb12.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    \n",
    "    plt.savefig('plots_test_2it_500k_tanh_D30_p1dropout_cosl_sqrtweights_sigmaleq_'+str(thresh)+'.png')\n",
    "    #np.save('G_train_2it_nn_500k_tanh_D30_p1dropout_cosl_log2dweights_sigmaleq_'+str(thresh)+'.npy',test_preds)\n",
    "    clb1.remove()\n",
    "    clb2.remove()\n",
    "    clb3.remove()\n",
    "    clb4.remove()\n",
    "    clb5.remove()\n",
    "    clb6.remove()\n",
    "    clb7.remove()\n",
    "    clb8.remove()\n",
    "    clb9.remove()\n",
    "    clb10.remove()\n",
    "    clb11.remove()\n",
    "    clb12.remove()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Going to try to find original indices of points with low error in train, validation, and test & retrain without those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_indices(thresh):\n",
    "    if thresh >= 80:\n",
    "        list_err_gt = []   \n",
    "        list_err_gt = [(data_test['radial_velocity']).values[i] for i in range(len(test_preds_2[:,1])) if test_preds_2[i,1] > thresh]\n",
    "        print(len(list_err_gt))\n",
    "        #now need indices of these values in data\n",
    "        indices = []\n",
    "        for i in range(len(list_err_gt)):\n",
    "            indices.append(data_test[data_test['radial_velocity']==(list_err_gt[i])].index[0])\n",
    "        np.save('data_indices_error_gt_'+str(thresh),indices)\n",
    "    else:\n",
    "        print('less')\n",
    "        list_err_lt = []   \n",
    "        list_err_lt = [(data_test['radial_velocity']).values[i] for i in range(len(test_preds_2[:,1])) if test_preds_2[i,1] < thresh]\n",
    "        print(len(list_err_lt))\n",
    "        #now need indices of these values in data\n",
    "        indices = []\n",
    "        for i in range(len(list_err_lt)):\n",
    "            indices.append(data_test[data_test['radial_velocity']==(list_err_lt[i])].index[0])\n",
    "        np.save('data_indices_error_lt_'+str(thresh),indices)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "CombinedModel.load_weights('models/' + 'TrainingBoth_0138.hdf5')\n",
    "test_preds_train = CombinedModel.predict(X_train)\n",
    "\n",
    "list_rv_train_err_lt65 = [y_train[i,0] for i in range(len(test_preds_train[:,1])) if test_preds_train[i,1] < 90]\n",
    "for i in range(len(list_rv_train_err_lt65)):\n",
    "    indices.append(data[data['radial_velocity']==(list_rv_train_err_lt65[i])].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_preds_val = CombinedModel.predict(X_val)\n",
    "\n",
    "list_rv_val_err_lt65 = [y_val[i,0] for i in range(len(test_preds_val[:,1])) if test_preds_val[i,1] < 90]\n",
    "for i in range(len(list_rv_val_err_lt65)):\n",
    "    indices.append(data[data['radial_velocity']==(list_rv_val_err_lt65[i])].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ML",
   "language": "python",
   "name": "venv_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

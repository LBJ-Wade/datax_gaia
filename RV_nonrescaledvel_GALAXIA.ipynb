{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML to find radial velocities using GALAXIA sim - need to change dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import sys\n",
    "import gzip\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.stats import norm\n",
    "\n",
    "matplotlib.rcParams.update({'font.family':'cmr10','font.size': 13})\n",
    "matplotlib.rcParams['axes.unicode_minus']=False\n",
    "matplotlib.rcParams['axes.labelsize']=15\n",
    "plt.rcParams['figure.figsize']=(4,4)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "plt.rcParams['mathtext.fontset'] = 'cm'\n",
    "plt.rcParams['mathtext.rm'] = 'serif'\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['xtick.top'] = True\n",
    "plt.rcParams['ytick.right'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/tigress/ljchang/DataXGaia/data/galaxia_mock/processed_stars_reshuffled_with_pos_vel.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = ['source_id', 'l', 'b', 'ra', 'dec', 'parallax', 'parallax_error', \n",
    "             'pmra', 'pmra_error', 'pmdec', 'pmdec_error', 'radial_velocity',\n",
    "             'photo_g_mean_mag', 'photo_bp_mean_mag', 'photo_rp_mean_mag',\n",
    "             'x','y','z','vx','vy','vz','r','phi','theta','vr','vphi','vtheta']\n",
    "#could train on radial_velocity_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns=data_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.drop(data[data.AccretedLabel > 0.0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indices_to_drop = np.load('data_indices_error_lt100_50k.npy')\n",
    "print(np.shape(indices_to_drop))\n",
    "print(indices_to_drop[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = data.drop(data.index[indices_to_drop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove 62,035 stars labeled as accreted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.parallax[data.parallax < 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove 484569 stars with radial velocity greater than 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.drop(data[data.radial_velocity < -100.0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What columns to use?\n",
    "use_cols = ['l', 'b','parallax','pmra','pmdec']\n",
    "# Make the design matrix\n",
    "X = data[use_cols].values\n",
    "Y = (data['radial_velocity']).values\n",
    "#Y = (Y - np.mean(Y))/(np.std(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is giving me a negative output because it's summing over y_train, which is not the number of stars, but is instead the radial velocities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, X_test, Y_train, y_test = train_test_split(X[0:500000], Y[0:500000], #44590\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=42)\n",
    "#0:497448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('truth_values_nontransformed',np.hstack((X_test,y_test.reshape(-1, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS = StandardScaler()\n",
    "\n",
    "x_train_save = x_train\n",
    "X_val_save = x_train_save[320001:400000,:]\n",
    "Y_train_save = Y_train\n",
    "X_test_save = X_test\n",
    "#max_x_train_cols = np.max(x_train)\n",
    "\n",
    "#x_train = np.divide(x_train, max_x_train_cols)\n",
    "#X_test = np.divide(X_test, max_x_train_cols)\n",
    "\n",
    "#Y_train_max = np.max(Y_train)\n",
    "#Y_train = Y_train.reshape(-1, 1)/Y_train_max\n",
    "#y_test = y_test.reshape(-1, 1)/Y_train_max\n",
    "\n",
    "x_train = SS.fit_transform(x_train)\n",
    "# Now scale the test data using the same mean and standard deviation as found with the training data\n",
    "\n",
    "X_test = SS.transform(X_test)\n",
    "\n",
    "SS_rv = StandardScaler()\n",
    "Y_train = SS_rv.fit_transform(Y_train.reshape(-1, 1))\n",
    "y_test = SS_rv.transform(y_test.reshape(-1, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(Y_train))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train[0:320000,:] #318336\n",
    "y_train = Y_train[0:320000]\n",
    "X_val = x_train[320001:400000,:]#397927\n",
    "y_val = Y_train[320001:400000]\n",
    "#need to split for validation\n",
    "y_low = -700\n",
    "y_high = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print()\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print()\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val[0:5,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying to add some weights to training data\n",
    "#might weight the mean, but no weights on confidence model? (something to think about)\n",
    "vr_real = Y_train_save\n",
    "counts, bins = np.histogram(vr_real,bins=np.linspace(-700,700,51))\n",
    "bin_centers = (bins[1:]+bins[:-1])/2\n",
    "interp_func  = interp1d(bin_centers,(counts).astype('float'))\n",
    "inv_weights = interp_func(vr_real)\n",
    "weights = 1/inv_weights\n",
    "weights = np.log(weights)\n",
    "weights = weights - np.min(weights)+1\n",
    "\n",
    "weights_train = weights[0:320000]\n",
    "weights_val = weights[320001:400000]\n",
    "weights_train = weights_train.reshape(-1, 1)\n",
    "weights_val = weights_val.reshape(-1, 1)\n",
    "print(np.shape(weights_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#visualize check data\n",
    "for i, col in enumerate(use_cols):\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist(data[col], bins=100)\n",
    "    plt.yscale('log')\n",
    "    plt.title('Data')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(X_train[:, i], bins=100, alpha=0.5)\n",
    "    plt.hist(X_test[:, i], bins=100, alpha=0.5)\n",
    "    plt.yscale('log')\n",
    "    plt.title('Scaled')\n",
    "    \n",
    "    plt.suptitle(col, y=1.01, fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train, bins=100, range=(-5,5), histtype='step', color= 'blue', label = 'vlos', alpha = 0.5, density = True)\n",
    "plt.hist(-y_train, bins=100, range=(-5,5), histtype='step', color= 'magenta',  label = '-vlos', alpha = 0.5, density = True)\n",
    "plt.hist(y_train, bins=100, weights = weights_train, range=(-5,5), histtype='step', color= 'orange', label = 'vlos weighted', density = True)\n",
    "plt.hist(-y_train, bins=100, weights = weights_train, range=(-5,5), histtype='step', color= 'red', label = '-vlos weighted', density = True)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import callbacks as callbacks\n",
    "global index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LikelihoodLossFunction(y_true, y_pred):\n",
    "    # shape of y_pred should be (nsamples, 2)\n",
    "    # the first column should be the mean of the prediction\n",
    "    # the second column is the confidence (number of standard deviations)\n",
    "#     print y_true.shape\n",
    "#     print y_pred.shape\n",
    "    SIGMA = K.abs(y_pred[:, 1]) + 1e-6\n",
    "    LOC = y_pred[:, 0]\n",
    "    X = y_true[:, 0]\n",
    "    weights = y_true[:,1]\n",
    "    ARG = K.square(X - LOC) / (2 * K.square(SIGMA))\n",
    "    PREFACT = K.log(K.pow(2 * np.pi * K.square(SIGMA), -0.5))\n",
    "    return K.mean((ARG - PREFACT) * weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConstantLikelihoodLossFunction(y_true, y_pred):\n",
    "    # shape of y_pred should be (nsamples, 2)\n",
    "    # the first column should be the mean of the prediction\n",
    "    # the second column is the confidence (number of standard deviations)\n",
    "#     print y_true.shape\n",
    "#     print y_pred.shape\n",
    "    LOC = y_pred[:, 0]\n",
    "    X = y_true[:, 0]\n",
    "    weights = y_true[:,1]\n",
    "    ARG = K.square(X - LOC) / (2.0)\n",
    "    PREFACT = K.log(K.pow(2 * np.pi, -0.5))\n",
    "    return K.mean((ARG - PREFACT) * weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two network technique to calculate the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Concatenate, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import clear_output\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.hstack([y_train, weights_train])\n",
    "y_val = np.hstack([y_val, weights_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del(inputs, MeanEst, MeanModel, ConfEst, ConfModel)\n",
    "inputs = Input(shape=(5,))\n",
    "MeanEst = (Dense(30, activation='tanh'))(inputs)\n",
    "MeanEst = (Dropout(0.1))(MeanEst)\n",
    "MeanEst = (Dense(30, activation='tanh'))(MeanEst)\n",
    "MeanEst = (Dropout(0.1))(MeanEst)\n",
    "MeanEst = (Dense(30, activation='tanh'))(MeanEst)\n",
    "MeanEst = (Dropout(0.1))(MeanEst)\n",
    "MeanEst = (Dense(30, activation='tanh'))(MeanEst)\n",
    "MeanEst = (Dropout(0.1))(MeanEst)\n",
    "MeanEst = (Dense(1, activation='linear'))(MeanEst)\n",
    "MeanModel = Model(inputs=[inputs], outputs=MeanEst)\n",
    "\n",
    "ConfEst= (Dense(30, activation='tanh'))(inputs)\n",
    "ConfEst = (Dropout(0.1))(ConfEst)\n",
    "ConfEst= (Dense(30, activation='tanh'))(ConfEst)\n",
    "ConfEst = (Dropout(0.1))(ConfEst)\n",
    "ConfEst= (Dense(30, activation='tanh'))(ConfEst)\n",
    "ConfEst = (Dropout(0.1))(ConfEst)\n",
    "ConfEst= (Dense(30, activation='tanh'))(ConfEst)\n",
    "ConfEst = (Dropout(0.1))(ConfEst)\n",
    "ConfEst= (Dense(1, activation='relu'))(ConfEst)\n",
    "ConfModel = Model(inputs=[inputs], outputs=ConfEst)\n",
    "#how can this give me a confidence output? the only thing that appears to define a confidence is the relu\n",
    "#on the last layer...think about this.\n",
    "#CombinedSub = Concatenate(axis=-1)([MeanEst, ConfEst])\n",
    "CombinedSub = Concatenate(axis=-1)([MeanModel(inputs), ConfModel(inputs)])\n",
    "\n",
    "CombinedModel = Model(inputs=[inputs], outputs=CombinedSub)\n",
    "#plot_model(CombinedModel,to_file='demo.png',show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CombinedModel.save_weights('modelsIterate/TrainingBoth_0000.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CombinedModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPoint = ModelCheckpoint('models_GALAXIA_nonn/TrainingMean_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR = ReduceLROnPlateau(patience=10, min_lr=1e-5, verbose=True)\n",
    "CSV_logger = CSVLogger(filename='logs/training_mean.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks = [CheckPoint, ES, RLR, CSV_logger]\n",
    "#mycallbacks = [ES, RLR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfModel.trainable = False\n",
    "MeanModel.trainable = True\n",
    "CombinedModel.compile(loss=ConstantLikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Model (Constant Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPoint2 = ModelCheckpoint('models_GALAXIA_nonn/TrainingErrorBars_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES2 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR2 = ReduceLROnPlateau(patience=10, min_lr=1e-5)\n",
    "CSV_logger2 = CSVLogger(filename='logs/training_errorbars.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks2 = [CheckPoint2, ES2, RLR2, CSV_logger2]\n",
    "#mycallbacks2 = [ES2, RLR2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfModel.trainable = True\n",
    "MeanModel.trainable = False\n",
    "CombinedModel.compile(loss=LikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks2\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Confidence Model (Likelihood Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPoint4 = ModelCheckpoint('models_GALAXIA_nonn/TrainingMean2_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES4 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR4 = ReduceLROnPlateau(patience=10, min_lr=1e-5, verbose=True)\n",
    "CSV_logger4 = CSVLogger(filename='logs/training_mean2.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks4 = [CheckPoint4, ES4, RLR4, CSV_logger4]\n",
    "#mycallbacks4 = [ES4, RLR4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfModel.trainable = False\n",
    "MeanModel.trainable = True\n",
    "CombinedModel.compile(loss=ConstantLikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks4\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Model (2nd Train) (Constant Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPoint5 = ModelCheckpoint('models_GALAXIA_nonn/TrainingErrorBars2_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES5 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR5 = ReduceLROnPlateau(patience=10, min_lr=1e-5)\n",
    "CSV_logger5 = CSVLogger(filename='logs/training_errorbars2.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks5 = [CheckPoint5, ES5, RLR5, CSV_logger5]\n",
    "#mycallbacks5 = [ES5, RLR5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfModel.trainable = True\n",
    "MeanModel.trainable = False\n",
    "CombinedModel.compile(loss=LikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks5\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Confidence Model (2nd Train) (Likelihood Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "CheckPoint6 = ModelCheckpoint('models_GALAXIA_nonn/TrainingMean3_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES4 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR4 = ReduceLROnPlateau(patience=10, min_lr=1e-5, verbose=True)\n",
    "CSV_logger6 = CSVLogger(filename='logs/training_mean3.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks6 = [CheckPoint6, ES4, RLR4, CSV_logger6]\n",
    "#mycallbacks4 = [ES4, RLR4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ConfModel.trainable = False\n",
    "MeanModel.trainable = True\n",
    "CombinedModel.compile(loss=ConstantLikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks6\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Model (3rd Train) (Constant Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "CheckPoint7 = ModelCheckpoint('models_GALAXIA_nonn/TrainingErrorBars3_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES5 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR5 = ReduceLROnPlateau(patience=10, min_lr=1e-5)\n",
    "CSV_logger7 = CSVLogger(filename='logs/training_errorbars3.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks7 = [CheckPoint7, ES5, RLR5, CSV_logger7]\n",
    "#mycallbacks5 = [ES5, RLR5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ConfModel.trainable = True\n",
    "MeanModel.trainable = False\n",
    "CombinedModel.compile(loss=LikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks7,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Confidence Model (3rd Train) (Likelihood Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CheckPoint3 = ModelCheckpoint('models_GALAXIA_nonn/TrainingBoth_{epoch:04d}.hdf5',\n",
    "                             verbose=0,\n",
    "                             save_best_only=False\n",
    "                            )\n",
    "ES3 = EarlyStopping(patience=40, verbose=True, restore_best_weights=True)\n",
    "RLR3 = ReduceLROnPlateau(patience=10, min_lr=1e-5)\n",
    "CSV_logger3 = CSVLogger(filename='logs/training_both.log', separator=',', append=False)\n",
    "\n",
    "mycallbacks3 = [CheckPoint3, ES3, RLR3, CSV_logger3]\n",
    "#mycallbacks3 = [ES3, RLR3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfModel.trainable = True\n",
    "MeanModel.trainable = True\n",
    "CombinedModel.compile(loss=LikelihoodLossFunction,\n",
    "                      optimizer='adam'\n",
    "                     )\n",
    "\n",
    "history = CombinedModel.fit(X_train,y_train,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  epochs=1000,\n",
    "                  batch_size=10000,\n",
    "                  callbacks = mycallbacks3\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "#plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Both Models (Likelihood Loss Function)')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.savefig('Loss_Plot_Iterate.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    " def plot_validation(i,type1):   \n",
    "    y_low = -700\n",
    "    y_high = 700\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    gs0 = gridspec.GridSpec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    from matplotlib.colors import LogNorm\n",
    "    plt.subplot(gs0[3])\n",
    "    plt.hist2d( test_preds[:,1],test_preds[:,0], bins=40,norm = LogNorm())\n",
    "    clb1 = plt.colorbar()\n",
    "    clb1.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel('$\\sigma$',labelpad=-10)\n",
    "\n",
    "    plt.subplot(gs0[4])\n",
    "    sc = plt.scatter(y_val[:,0], test_preds[:,0], c =test_preds[:,1], label = 'data',marker = '.',cmap=plt.cm.YlOrRd)\n",
    "    x1 = np.linspace(y_low,y_high,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.xlim([y_low, y_high])\n",
    "    plt.ylim([y_low,y_high])\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    clb2 = plt.colorbar(sc)\n",
    "    clb2.set_label('$\\sigma$', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.clim(0,750)\n",
    "\n",
    "    plt.subplot(gs0[2])\n",
    "    hb = plt.hexbin(y_val[:,0], test_preds[:,0],gridsize=80, norm = LogNorm(), extent=[-150, 150, -150, 150])\n",
    "    x1 = np.linspace(np.min(y_val[:,0]),np.max(y_val[:,0]),1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    clb3 = plt.colorbar(hb)\n",
    "    clb3.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "\n",
    "\n",
    "    plt.subplot(gs0[5])\n",
    "    sc = plt.scatter(y_val[:,0], test_preds[:,0], c =test_preds[:,1], label = 'data',marker = '.',cmap=plt.cm.YlOrRd)\n",
    "    x1 = np.linspace(y_low,y_high,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.xlim([y_low, y_high])\n",
    "    plt.ylim([y_low,y_high])\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    clb4 = plt.colorbar(sc)\n",
    "    clb4.set_label('$\\sigma$\\n(saturated)', labelpad=-25, y=1.15, rotation=0,fontsize=10)\n",
    "    plt.clim(0,200)\n",
    "\n",
    "    plt.subplot(gs0[6])\n",
    "    dist_hist = np.divide(np.ones_like(X_test_save[:,2]),X_val_save[:,2]*1000)\n",
    "    plt.hist2d(dist_hist, test_preds_2[:,1], bins=40, norm = LogNorm())\n",
    "    clb5 = plt.colorbar()\n",
    "    clb5.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.xlabel('Distance',fontsize = 12)\n",
    "    plt.ylabel('$\\sigma$',labelpad=-5)\n",
    "\n",
    "    plt.subplot(gs0[1])\n",
    "    plotrange = np.linspace(-5,5,1000)\n",
    "    diff_hist = np.divide(np.subtract(test_preds[:,0],y_val[:,0]),test_preds[:,1])\n",
    "    mean_diffs, mean_stds = np.mean(diff_hist), np.std(diff_hist)\n",
    "    plt.hist(diff_hist,bins=20, range=(-5,5), histtype='step',color = 'lightseagreen',linewidth = 1.5, density = True)\n",
    "    plt.plot(plotrange, norm.pdf(plotrange, mean_diffs, mean_stds),color = 'darkorange', linestyle = '--', linewidth = 2.5,label = 'normal fit')\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 8})\n",
    "    plt.xlabel(r'$(v_{\\rm{los}}^{\\rm{pred}} - v_{\\rm{los}}^{\\rm{meas}})/\\sigma$',labelpad=-5)\n",
    "    \n",
    "    plt.subplot(gs0[7])\n",
    "    diff_hist_err = np.subtract(test_preds[:,0],y_val[:,0])\n",
    "    plt.hist2d(test_preds[:,1],diff_hist_err, bins=40,norm = LogNorm())\n",
    "    clb6 = plt.colorbar()\n",
    "    clb6.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$(v_{\\rm{los}}^{\\rm{pred}} - v_{\\rm{los}}^{\\rm{meas}})$',labelpad=-5)\n",
    "    plt.xlabel('$\\sigma$',labelpad=-5)\n",
    "\n",
    "    plt.subplot(gs0[0])\n",
    "    plt.hist(y_val[:,0], bins=25, range=(y_low,y_high), histtype='step', edgecolor = 'skyblue', color= 'skyblue', fill = True, label = 'val' )\n",
    "    plt.hist(test_preds[:,0], bins=25, range=(y_low,y_high), histtype='step',color = 'blue', label = 'predicted')\n",
    "    plt.xlabel(r'$v_{\\rm{los}}$', labelpad =-2)\n",
    "    plt.title('Validation set, Train '+type1+', Epoch '+str(i)+', 20 bins',fontsize=14)\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 10})\n",
    "    \n",
    "    plt.savefig('plots'+type1+'/'+'train'+type1+'_'+str(i)+'.png')\n",
    "    clb1.remove()\n",
    "    clb2.remove()\n",
    "    clb3.remove()\n",
    "    clb4.remove()\n",
    "    clb5.remove()\n",
    "    clb6.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting First Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininmean = [file for file in os.listdir('models_GALAXIA_nonn/')\n",
    "               if file.startswith('TrainingMean_')]\n",
    "traininmean = sorted(traininmean)[1::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i, mfile in enumerate(traininmean):\n",
    "    CombinedModel.load_weights('models_GALAXIA_nonn/' + mfile)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i*10,'Mean1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininerror = [file for file in os.listdir('models_GALAXIA_nonn/')\n",
    "               if file.startswith('TrainingErrorBars_')]\n",
    "traininerror = sorted(traininerror)[0::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i_err, mfile_err in enumerate(traininerror):\n",
    "    CombinedModel.load_weights('models_GALAXIA_nonn/' + mfile_err)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i_err*20,'Error1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Plotting Second Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininmean2 = [file for file in os.listdir('models_GALAXIA_nonn/')\n",
    "               if file.startswith('TrainingMean2')]\n",
    "traininmean2 = sorted(traininmean2)[0::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i, mfile in enumerate(traininmean2):\n",
    "    CombinedModel.load_weights('models_GALAXIA_nonn/' + mfile)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i*10,'Mean2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininerror2 = [file for file in os.listdir('models_GALAXIA_nonn/')\n",
    "               if file.startswith('TrainingErrorBars2')]\n",
    "traininerror2 = sorted(traininerror2)[0::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i_err, mfile_err in enumerate(traininerror2):\n",
    "    CombinedModel.load_weights('models_GALAXIA_nonn/' + mfile_err)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i_err*20,'Error2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Plotting Third Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininmean3 = [file for file in os.listdir('models_GALAXIA_nonn/')\n",
    "               if file.startswith('TrainingMean3')]\n",
    "traininmean3 = sorted(traininmean3)[0::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i, mfile in enumerate(traininmean3):\n",
    "    CombinedModel.load_weights('models_GALAXIA_nonn/' + mfile)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i*10,'Mean3');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininerror3 = [file for file in os.listdir('models_GALAXIA_nonn/')\n",
    "               if file.startswith('TrainingErrorBars3')]\n",
    "traininerror3 = sorted(traininerror3)[0::20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i_err, mfile_err in enumerate(traininerror3):\n",
    "    CombinedModel.load_weights('models_GALAXIA_nonn/' + mfile_err)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i_err*20,'Error3');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Plot results of training both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "traininboth = [file for file in os.listdir('models_GALAXIA_nonn/')\n",
    "               if file.startswith('TrainingBoth')]\n",
    "traininboth = sorted(traininboth)[1::10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "for i_both, mfile_both in enumerate(traininboth):\n",
    "    CombinedModel.load_weights('models_GALAXIA_nonn/' + mfile_both)\n",
    "    \n",
    "    test_preds = CombinedModel.predict(X_val)\n",
    "\n",
    "    \n",
    "    plot_validation(i_both*10,'Both');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(np.size(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CombinedModel.load_weights('G_train_2it_50k/models_GALAXIA_nonn/' + 'TrainingBoth_0109.hdf5')\n",
    "test_preds_2 = CombinedModel.predict(X_test)\n",
    "y_test = SS_rv.inverse_transform(y_test)\n",
    "test_preds_2[:,0] = SS_rv.inverse_transform(test_preds_2[:,0])\n",
    "test_preds_2[:,1] = SS_rv.inverse_transform(test_preds_2[:,1])\n",
    "y_test = y_test*Y_train_max\n",
    "test_preds_2[:,0] = test_preds_2[:,0]*Y_train_max\n",
    "test_preds_2[:,1] = test_preds_2[:,1]*Y_train_max\n",
    "\n",
    "y_low = -250\n",
    "y_high = 250\n",
    "plt.figure()\n",
    "plt.hist(y_test, bins=50, range=(y_low,y_high), histtype='step', edgecolor = 'skyblue', color= 'skyblue', fill = True, label = 'test', density = True)\n",
    "plt.hist(test_preds_2[:,0], bins=50, range=(y_low,y_high), histtype='step',color = 'blue', label = 'predicted', density = True)\n",
    "plt.xlabel('Vr')\n",
    "plt.ylabel('Stars')\n",
    "plt.title('Test set - simulated data')\n",
    "#plt.yscale('log')\n",
    "plt.legend(loc = \"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going to use Monte Carlo simulations to predict errorbars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "mc_vr_pred_list = []\n",
    "bin_values_list = []\n",
    "min_array = []\n",
    "max_array = []\n",
    "mean_array = []\n",
    "for mc_i in range(0,1000):\n",
    "    #print(mc_i)\n",
    "    mc_vr_pred = []\n",
    "    for star_i in range(0,np.size(y_test)):\n",
    "        #mc_vr_pred.append(np.mean([np.random.normal(test_preds_2[star_i,0],test_preds_2[star_i,1]) for _ in range(1000)]))\n",
    "        mc_vr_pred.append(np.random.normal(test_preds_2[star_i,0],test_preds_2[star_i,1]))\n",
    "    mc_vr_pred_list.append(mc_vr_pred)\n",
    "    n, bins, patches = plt.hist(mc_vr_pred, bins=20, range=(y_low,y_high), histtype='step')\n",
    "    plt.xlabel('Vr Monte Carlo')\n",
    "    plt.yscale('log')\n",
    "    bin_values_list.append(n)\n",
    "bin_values_list_arr = np.array(bin_values_list)    \n",
    "max_array = bin_values_list_arr.max(axis=0)\n",
    "min_array = bin_values_list_arr.min(axis=0) \n",
    "mean_array = bin_values_list_arr.mean(axis=0)\n",
    "y_error = [min_array,max_array]\n",
    "print(np.shape(y_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "hist_test, bins_test, patches_test = plt.hist(y_test, bins=20, range=(y_low,y_high), histtype='step', edgecolor = 'skyblue', color= 'skyblue', fill = True, label = 'test' , zorder = 0)\n",
    "#plt.hist(bins_test[:-1], bins_test, weights=max_array,color = 'green')\n",
    "#plt.hist(bins_test[:-1], bins_test, weights=min_array,color = 'yellow')\n",
    "bin_centers_test = (bins_test[1:]+bins_test[:-1])/2\n",
    "plt.fill_between(bin_centers_test,min_array, max_array,label = 'MC spread',color = 'orange', zorder = 10)\n",
    "\n",
    "plt.hist(test_preds_2[:,0], bins=20, range=(y_low,y_high), histtype='step',color = 'blue', label = 'predicted', zorder = 20)\n",
    "print(y_low,y_high)\n",
    "plt.xlabel(r'$v_{\\rm{los}}$')\n",
    "plt.title('Test set - simulated data')\n",
    "plt.yscale('log')\n",
    "plt.legend(loc = \"upper right\",prop={'size': 10})\n",
    "plt.savefig('MC_100trials_8000stars_noavg.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def plot_test():   \n",
    "    y_low = -200\n",
    "    y_high = 200\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    gs0 = gridspec.GridSpec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    from matplotlib.colors import LogNorm\n",
    "    plt.subplot(gs0[3])\n",
    "    plt.hist2d(test_preds_2[:,1],test_preds_2[:,0], bins=40,norm = LogNorm())\n",
    "    clb1 = plt.colorbar()\n",
    "    clb1.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel('$\\sigma$',labelpad=-5)\n",
    "    \n",
    "\n",
    "    plt.subplot(gs0[4])\n",
    "    sc = plt.scatter(y_test[:,0], test_preds_2[:,0], c =test_preds_2[:,1], label = 'data',marker = '.',cmap=plt.cm.YlOrRd)\n",
    "    x1 = np.linspace(y_low,y_high,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.xlim([y_low, y_high])\n",
    "    plt.ylim([y_low,y_high])\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    clb2 = plt.colorbar(sc)\n",
    "    clb2.set_label('$\\sigma$', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.clim(0,750)\n",
    "\n",
    "    plt.subplot(gs0[2])\n",
    "    hb = plt.hexbin(y_test[:,0], test_preds_2[:,0],gridsize=80, norm = LogNorm(), extent = [-150,150,-150,150])\n",
    "    x1 = np.linspace(np.min(test_preds_2[:,0]),np.max(test_preds_2[:,0]),1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    clb3 = plt.colorbar(hb)\n",
    "    clb3.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "\n",
    "\n",
    "    plt.subplot(gs0[5])\n",
    "    sc = plt.scatter(y_test[:,0], test_preds_2[:,0], c =test_preds_2[:,1], label = 'data',marker = '.',cmap=plt.cm.YlOrRd)\n",
    "    x1 = np.linspace(y_low,y_high,1000)\n",
    "    y1 = x1\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.xlim([y_low, y_high])\n",
    "    plt.ylim([y_low,y_high])\n",
    "    plt.ylabel(r'$v_{\\rm{los}}^{\\rm{pred}}$',labelpad=-10)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}^{\\rm{meas}}$')\n",
    "    clb4 = plt.colorbar(sc)\n",
    "    clb4.set_label('$\\sigma$\\n(saturated)', labelpad=-25, y=1.15, rotation=0,fontsize=10)\n",
    "    plt.clim(0,200)\n",
    "\n",
    "    plt.subplot(gs0[6])\n",
    "    dist_hist = np.divide(np.ones_like(X_test_save[:,2]),X_test_save[:,2])\n",
    "    plt.hist2d(dist_hist, test_preds_2[:,1], bins=40, norm = LogNorm())\n",
    "    clb5 = plt.colorbar()\n",
    "    clb5.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.xlabel('Distance (kpc)',fontsize = 12)\n",
    "    plt.ylabel('$\\sigma$',labelpad=-5)\n",
    "\n",
    "    plt.subplot(gs0[1])\n",
    "    plotrange = np.linspace(-5,5,1000)\n",
    "    diff_hist = np.divide(np.subtract(test_preds_2[:,0],y_test[:,0]),test_preds_2[:,1])\n",
    "    mean_diffs, mean_stds = np.mean(diff_hist), np.std(diff_hist)\n",
    "    plt.hist(diff_hist,bins=20, range=(-5,5), histtype='step',color = 'lightseagreen',linewidth = 1.5, density = True)\n",
    "    plt.plot(plotrange, norm.pdf(plotrange, mean_diffs, mean_stds),color = 'darkorange', linestyle = '--', linewidth = 2.5,label = 'normal fit')\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 8})\n",
    "    plt.xlabel(r'$(v_{\\rm{los}}^{\\rm{pred}} - v_{\\rm{los}}^{\\rm{meas}})/\\sigma$',labelpad=-5)\n",
    "    \n",
    "    plt.subplot(gs0[7])\n",
    "    plt.hist2d(X_test_save[:,0],test_preds_2[:,1], bins=40,norm = LogNorm())\n",
    "    clb6 = plt.colorbar()\n",
    "    clb6.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.xlabel(r'$l$',labelpad=-3)\n",
    "    plt.ylabel('$\\sigma$',labelpad=-5)\n",
    "    \n",
    "    plt.subplot(gs0[8])\n",
    "    plt.hist2d(X_test_save[:,1],test_preds_2[:,1], bins=40,norm = LogNorm())\n",
    "    clb6 = plt.colorbar()\n",
    "    clb6.set_label('Density', labelpad=-25, y=1.08, rotation=0,fontsize=10)\n",
    "    plt.plot(x1,y1,'k--')\n",
    "    plt.xlabel(r'$b$',labelpad=-3)\n",
    "    plt.ylabel('$\\sigma$',labelpad=-5)\n",
    "\n",
    "    plt.subplot(gs0[0])\n",
    "    plt.hist(y_test, bins=50, range=(y_low,y_high), histtype='step', edgecolor = 'skyblue', color= 'skyblue', fill = True, label = 'test', density = True )\n",
    "    plt.hist(test_preds_2[:,0], bins=50, range=(y_low,y_high), histtype='step',color = 'blue', label = 'predicted', density = True)\n",
    "    plt.xlabel(r'$v_{\\rm{los}}$', labelpad =-2)\n",
    "    plt.title('Test set - simulated data, 50 bins',fontsize=14)\n",
    "    #plt.yscale('log')\n",
    "    plt.legend(loc = \"upper right\",prop={'size': 10})\n",
    "    plt.savefig('G_plots_test_2it_500k_tanh_NORM1st_try2.png')\n",
    "    #plt.savefig('plots_error/trainerror_'+str(i)+'.png')\n",
    "    clb1.remove()\n",
    "    clb2.remove()\n",
    "    clb3.remove()\n",
    "    clb4.remove()\n",
    "    clb5.remove()\n",
    "    clb6.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Going to try to find original indices of points with low error in train, validation, and test & retrain without those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    " \n",
    "# Make the plot\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_trisurf(X_test_save[:,0], X_test_save[:,1], test_preds_2[:,1], cmap=plt.cm.YlOrRd, linewidth=0.2)\n",
    "ax.set_xlabel('$l$')\n",
    "ax.set_ylabel('$b$')\n",
    "ax.set_zlabel('$\\sigma$',rotation=0)\n",
    "ax.view_init(azim=-30, elev=50)\n",
    "surf=ax.plot_trisurf(X_test_save[:,0], X_test_save[:,1], test_preds_2[:,1], cmap=plt.cm.YlOrRd, linewidth=0.2)\n",
    "fig.colorbar( surf, shrink=0.5, aspect=5, label = '$\\sigma$')\n",
    "plt.show()\n",
    " \n",
    "# to Add a color bar which maps values to colors.\n",
    "surf=ax.plot_trisurf(X_test_save[:,0], X_test_save[:,1], test_preds_2[:,1], cmap=plt.cm.YlOrRd, linewidth=0.2)\n",
    "fig.colorbar( surf, shrink=0.5, aspect=5)\n",
    "plt.show()\n",
    "\n",
    " \n",
    "# Other palette\n",
    "ax.plot_trisurf(X_test_save[:,0], X_test_save[:,1], test_preds_2[:,1], cmap=plt.cm.YlOrRd, linewidth=0.01)\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "data=np.vstack((X_test_save[:,0], X_test_save[:,1], test_preds_2[:,1]))\n",
    "data = np.transpose(data)\n",
    "values = data.T\n",
    "kde = stats.gaussian_kde(values)\n",
    "density = kde(values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca(projection='3d')\n",
    "x, y, z = values\n",
    "surf1 = ax.scatter(x, y, z, c=density)\n",
    "ax.set_xlabel('$l$')\n",
    "ax.set_ylabel('$b$')\n",
    "ax.set_zlabel('$\\sigma$',rotation=0)\n",
    "ax.view_init(azim=-60, elev=10)\n",
    "fig.colorbar( surf1, shrink=0.5, aspect=5, label = 'density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(test_preds_2[:,1]))\n",
    "\n",
    " # min_cl = np.nanmin(code_lengths)\n",
    "  #  min_k = Ks[code_lengths.index(min_cl)] \n",
    "list_pmra_test_err_lt65 = []   \n",
    "list_rv_test_err_lt65 = [y_test[i,0] for i in range(len(test_preds_2[:,1])) if test_preds_2[i,1] < 90]\n",
    "print(len(list_rv_test_err_lt65))\n",
    "#now need indices of these values in data\n",
    "indices = []\n",
    "for i in range(len(list_rv_test_err_lt65)):\n",
    "    indices.append(data[data['radial_velocity']==(list_rv_test_err_lt65[i])].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CombinedModel.load_weights('models/' + 'TrainingBoth_0138.hdf5')\n",
    "test_preds_train = CombinedModel.predict(X_train)\n",
    "\n",
    "list_rv_train_err_lt65 = [y_train[i,0] for i in range(len(test_preds_train[:,1])) if test_preds_train[i,1] < 90]\n",
    "for i in range(len(list_rv_train_err_lt65)):\n",
    "    indices.append(data[data['radial_velocity']==(list_rv_train_err_lt65[i])].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_val = CombinedModel.predict(X_val)\n",
    "\n",
    "list_rv_val_err_lt65 = [y_val[i,0] for i in range(len(test_preds_val[:,1])) if test_preds_val[i,1] < 90]\n",
    "for i in range(len(list_rv_val_err_lt65)):\n",
    "    indices.append(data[data['radial_velocity']==(list_rv_val_err_lt65[i])].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data_indices_error_lt100_50k',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ML",
   "language": "python",
   "name": "venv_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
